{
    "has_issue": true,
    "overall_severity": "major",
    "issues": [
        {
            "short_description": "Use of deprecated model name",
            "detailed_explaination": "The model name 'gpt-4o' is not a recognized or valid model name in the latest OpenAI API specifications. It is likely a typo or an outdated reference.",
            "suggestion": "Update the model name to a valid one, such as 'gpt-4'.",
            "issue_line_begin": 15,
            "issue_line_end": 15,
            "issue_reference_urls": [
                "https://platform.openai.com/docs/models/gpt"
            ],
            "severity": "major",
            "severity_reason": "Using an invalid model name can lead to runtime errors and prevent the application from functioning as intended."
        }
    ],
    "fixed_code": "from typing import Literal\nimport asyncio\n\nfrom langchain_core.messages import HumanMessage, AIMessageChunk\nfrom langchain_openai import ChatOpenAI\n\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.types import Command\n\nfrom tools import make_supervisor_node\nfrom research_team import research_graph\nfrom paper_writing_team import paper_writing_graph\n\n##########################################################################################\n# Add Layers\n# \n# In this design, we are enforcing a top-down planning policy. We've created two graphs already, \n# but we have to decide how to route work between the two.\n#\n# We'll create a third graph to orchestrate the previous two, and add some connectors to \n# define how this top-level state is shared between the different graphs.\n\n\nllm = ChatOpenAI(model=\"gpt-4\")  # Updated model name\n\nteams_supervisor_node = make_supervisor_node(llm, [\"research_team\", \"writing_team\"])\n\ndef call_research_team(state: MessagesState) -\u003e Command[Literal[\"supervisor\"]]:\n    last_message = state[\"messages\"][-1]\n    response = research_graph.invoke({\"messages\": last_message})\n    \n    last_response = response[\"messages\"][-1].content\n    messages = [\n                HumanMessage(\n                    content=last_response, name=\"research_team\"\n                )\n            ]\n\n    return Command(\n        update={\n            \"messages\": messages\n        },\n        goto=\"supervisor\",\n    )\n\n\ndef call_paper_writing_team(state: MessagesState) -\u003e Command[Literal[\"supervisor\"]]:\n    last_message = state[\"messages\"][-1]\n    response = paper_writing_graph.invoke({\"messages\": last_message})\n\n    last_response = response[\"messages\"][-1].content\n    messages = [\n                HumanMessage(\n                    content=last_response, name=\"writing_team\"\n                )\n            ]\n\n    return Command(\n        update={\n            \"messages\": messages\n        },\n        goto=\"supervisor\",\n    )\n\n\n# Define the graph.\nsuper_builder = StateGraph(MessagesState)\nsuper_builder.add_node(\"supervisor\", teams_supervisor_node)\nsuper_builder.add_node(\"research_team\", call_research_team)\nsuper_builder.add_node(\"writing_team\", call_paper_writing_team)\n\nsuper_builder.add_edge(START, \"supervisor\")\nsuper_graph = super_builder.compile()\n\n\n# from IPython.display import Image\n\n# output_path = \"super_graph.png\" \n\n# png = Image(super_graph.get_graph().draw_mermaid_png())\n# png_data = png.data\n# with open(output_path, \"wb\") as file:\n#     file.write(png_data)\n\n# print(f\"Graph has been saved to {output_path}\")\n\nasync def test_graph():\n    async for messages in super_graph.astream(\n        {\n            \"messages\": [\n                (\"user\", \"Research AI agents and write a brief report about them.\")\n            ],\n        },\n        {\"recursion_limit\": 150},\n        stream_mode=\"messages\"\n    ):\n        #print(messages)\n        #print('------------------------')\n\n        checkpoint_ns:str = messages[1][\"checkpoint_ns\"]\n        is_cared_checkpoints = checkpoint_ns.startswith(\"search:\") or checkpoint_ns.startswith(\"note_taker:\")\n        if True or is_cared_checkpoints:\n            for msg in messages:\n                if isinstance(msg, AIMessageChunk):\n                    content = msg.content\n                    if content:\n                        print(content, end=\"\", flush=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(test_graph())\n    print()\n",
    "original_code": "from typing import Literal\nimport asyncio\n\nfrom langchain_core.messages import HumanMessage, AIMessageChunk\nfrom langchain_openai import ChatOpenAI\n\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.types import Command\n\nfrom tools import make_supervisor_node\nfrom research_team import research_graph\nfrom paper_writing_team import paper_writing_graph\n\n##########################################################################################\n# Add Layers\n# \n# In this design, we are enforcing a top-down planning policy. We've created two graphs already, \n# but we have to decide how to route work between the two.\n#\n# We'll create a third graph to orchestrate the previous two, and add some connectors to \n# define how this top-level state is shared between the different graphs.\n\n\nllm = ChatOpenAI(model=\"gpt-4o\")\n\nteams_supervisor_node = make_supervisor_node(llm, [\"research_team\", \"writing_team\"])\n\ndef call_research_team(state: MessagesState) -\u003e Command[Literal[\"supervisor\"]]:\n    last_message = state[\"messages\"][-1]\n    response = research_graph.invoke({\"messages\": last_message})\n    \n    last_response = response[\"messages\"][-1].content\n    messages = [\n                HumanMessage(\n                    content=last_response, name=\"research_team\"\n                )\n            ]\n\n    return Command(\n        update={\n            \"messages\": messages\n        },\n        goto=\"supervisor\",\n    )\n\n\ndef call_paper_writing_team(state: MessagesState) -\u003e Command[Literal[\"supervisor\"]]:\n    last_message = state[\"messages\"][-1]\n    response = paper_writing_graph.invoke({\"messages\": last_message})\n\n    last_response = response[\"messages\"][-1].content\n    messages = [\n                HumanMessage(\n                    content=last_response, name=\"writing_team\"\n                )\n            ]\n\n    return Command(\n        update={\n            \"messages\": messages\n        },\n        goto=\"supervisor\",\n    )\n\n\n# Define the graph.\nsuper_builder = StateGraph(MessagesState)\nsuper_builder.add_node(\"supervisor\", teams_supervisor_node)\nsuper_builder.add_node(\"research_team\", call_research_team)\nsuper_builder.add_node(\"writing_team\", call_paper_writing_team)\n\nsuper_builder.add_edge(START, \"supervisor\")\nsuper_graph = super_builder.compile()\n\n\n# from IPython.display import Image\n\n# output_path = \"super_graph.png\" \n\n# png = Image(super_graph.get_graph().draw_mermaid_png())\n# png_data = png.data\n# with open(output_path, \"wb\") as file:\n#     file.write(png_data)\n\n# print(f\"Graph has been saved to {output_path}\")\n\nasync def test_graph():\n    async for messages in super_graph.astream(\n        {\n            \"messages\": [\n                (\"user\", \"Research AI agents and write a brief report about them.\")\n            ],\n        },\n        {\"recursion_limit\": 150},\n        stream_mode=\"messages\"\n    ):\n        #print(messages)\n        #print('------------------------')\n\n        checkpoint_ns:str = messages[1][\"checkpoint_ns\"]\n        is_cared_checkpoints = checkpoint_ns.startswith(\"search:\") or checkpoint_ns.startswith(\"note_taker:\")\n        if True or is_cared_checkpoints:\n            for msg in messages:\n                if isinstance(msg, AIMessageChunk):\n                    content = msg.content\n                    if content:\n                        print(content, end=\"\", flush=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(test_graph())\n    print()\n",
    "path": "backend/graph.py",
    "model_usage_metrics": {
        "Duration": 14989560339,
        "OpenAiUsage": {
            "completion_tokens": 0,
            "prompt_tokens": 0,
            "total_tokens": 0,
            "completion_tokens_details": {
                "accepted_prediction_tokens": 0,
                "audio_tokens": 0,
                "reasoning_tokens": 0,
                "rejected_prediction_tokens": 0
            },
            "prompt_tokens_details": {
                "audio_tokens": 0,
                "cached_tokens": 0
            }
        }
    }
}