{
    "has_issue": true,
    "overall_severity": "minor",
    "issues": [
        {
            "short_description": "Use of deprecated model name.",
            "detailed_explaination": "The model name 'gpt-4o' is not recognized as a valid model name in the latest OpenAI API. It is likely a typo or an outdated reference. The correct model name should be 'gpt-4'.",
            "suggestion": "Change the model name from 'gpt-4o' to 'gpt-4'.",
            "issue_line_begin": 15,
            "issue_line_end": 15,
            "issue_reference_urls": [
                "https://platform.openai.com/docs/models/gpt-4"
            ],
            "severity": "minor",
            "severity_reason": "The issue is related to the use of an incorrect model name which may lead to runtime errors."
        }
    ],
    "fixed_code": "from typing import Literal\nimport asyncio\nfrom langchain_core.messages import HumanMessage, AIMessageChunk\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.types import Command\n\nfrom tools import tavily_tool, scrape_webpages, make_supervisor_node\n\n#######################################################################################################################\n# Define Agent Teams\n# Now we can get to define our hierarchical teams. \"Choose your player!\"\n\n# Research Team\n# The research team will have a search agent and a web scraping \"research_agent\" as the two worker nodes. \n# Let's create those, as well as the team research_team_supervisor.\n\nllm = ChatOpenAI(model=\"gpt-4\")  # Changed from \"gpt-4o\" to \"gpt-4\"\n\nsearch_agent = create_react_agent(llm, tools=[tavily_tool])\n\n\ndef search_node(state: MessagesState) -\u003e Command[Literal[\"research_team_supervisor\"]]:\n    result = search_agent.invoke(state)\n\n    last_response = result[\"messages\"][-1].content\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=last_response, name=\"search\")\n            ]\n        },\n        # We want our workers to ALWAYS \"report back\" to the research_team_supervisor when done\n        goto=\"research_team_supervisor\",\n    )\n\n\nweb_scraper_agent = create_react_agent(llm, tools=[scrape_webpages])\n\n\ndef web_scraper_node(state: MessagesState, config: RunnableConfig) -\u003e Command[Literal[\"research_team_supervisor\"]]:\n    result = web_scraper_agent.invoke(state, config)\n\n    last_response = result[\"messages\"][-1].content\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=last_response, name=\"web_scraper\")\n            ]\n        },\n        # We want our workers to ALWAYS \"report back\" to the research_team_supervisor when done\n        goto=\"research_team_supervisor\",\n    )\n\n\nresearch_supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n\n# Now that we've created the necessary components, defining their interactions is easy. \n# Add the nodes to the team graph, and define the edges, which determine the transition criteria.\n\nresearch_builder = StateGraph(MessagesState)\nresearch_builder.add_node(\"research_team_supervisor\", research_supervisor_node)\nresearch_builder.add_node(\"search\", search_node)\nresearch_builder.add_node(\"web_scraper\", web_scraper_node)\n\nresearch_builder.add_edge(START, \"research_team_supervisor\")\nresearch_graph = research_builder.compile()\n\n###############################################################################\n# from IPython.display import Image\n\n# output_path = \"research_graph.png\" \n\n# png = Image(research_graph.get_graph().draw_mermaid_png())\n# png_data = png.data\n# with open(output_path, \"wb\") as file:\n#     file.write(png_data)\n\n# print(f\"Graph has been saved to {output_path}\")\n\n###############################################################################\n# We can give this team work directly. Try it out below.\n\nasync def test_research_team():\n    async for messages in research_graph.astream(\n        {\"messages\": [(\"user\", \"when is Taylor Swift's next tour?\")]},\n        {\"recursion_limit\": 100},\n        stream_mode=\"messages\"\n    ):\n        checkpoint_ns:str = messages[1][\"checkpoint_ns\"]\n        if checkpoint_ns.startswith(\"search:\"):\n            for msg in messages:\n                if isinstance(msg, AIMessageChunk):\n                    content = msg.content\n                    if content:\n                        print(content, end=\"\", flush=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(test_research_team())\n    print()\n",
    "original_code": "from typing import Literal\nimport asyncio\nfrom langchain_core.messages import HumanMessage, AIMessageChunk\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.types import Command\n\nfrom tools import tavily_tool, scrape_webpages, make_supervisor_node\n\n#######################################################################################################################\n# Define Agent Teams\n# Now we can get to define our hierarchical teams. \"Choose your player!\"\n\n# Research Team\n# The research team will have a search agent and a web scraping \"research_agent\" as the two worker nodes. \n# Let's create those, as well as the team research_team_supervisor.\n\nllm = ChatOpenAI(model=\"gpt-4o\")\n\nsearch_agent = create_react_agent(llm, tools=[tavily_tool])\n\n\ndef search_node(state: MessagesState) -\u003e Command[Literal[\"research_team_supervisor\"]]:\n    result = search_agent.invoke(state)\n\n    last_response = result[\"messages\"][-1].content\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=last_response, name=\"search\")\n            ]\n        },\n        # We want our workers to ALWAYS \"report back\" to the research_team_supervisor when done\n        goto=\"research_team_supervisor\",\n    )\n\n\nweb_scraper_agent = create_react_agent(llm, tools=[scrape_webpages])\n\n\ndef web_scraper_node(state: MessagesState, config: RunnableConfig) -\u003e Command[Literal[\"research_team_supervisor\"]]:\n    result = web_scraper_agent.invoke(state, config)\n\n    last_response = result[\"messages\"][-1].content\n    return Command(\n        update={\n            \"messages\": [\n                HumanMessage(content=last_response, name=\"web_scraper\")\n            ]\n        },\n        # We want our workers to ALWAYS \"report back\" to the research_team_supervisor when done\n        goto=\"research_team_supervisor\",\n    )\n\n\nresearch_supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n\n# Now that we've created the necessary components, defining their interactions is easy. \n# Add the nodes to the team graph, and define the edges, which determine the transition criteria.\n\nresearch_builder = StateGraph(MessagesState)\nresearch_builder.add_node(\"research_team_supervisor\", research_supervisor_node)\nresearch_builder.add_node(\"search\", search_node)\nresearch_builder.add_node(\"web_scraper\", web_scraper_node)\n\nresearch_builder.add_edge(START, \"research_team_supervisor\")\nresearch_graph = research_builder.compile()\n\n###############################################################################\n# from IPython.display import Image\n\n# output_path = \"research_graph.png\" \n\n# png = Image(research_graph.get_graph().draw_mermaid_png())\n# png_data = png.data\n# with open(output_path, \"wb\") as file:\n#     file.write(png_data)\n\n# print(f\"Graph has been saved to {output_path}\")\n\n###############################################################################\n# We can give this team work directly. Try it out below.\n\nasync def test_research_team():\n    async for messages in research_graph.astream(\n        {\"messages\": [(\"user\", \"when is Taylor Swift's next tour?\")]},\n        {\"recursion_limit\": 100},\n        stream_mode=\"messages\"\n    ):\n        checkpoint_ns:str = messages[1][\"checkpoint_ns\"]\n        if checkpoint_ns.startswith(\"search:\"):\n            for msg in messages:\n                if isinstance(msg, AIMessageChunk):\n                    content = msg.content\n                    if content:\n                        print(content, end=\"\", flush=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(test_research_team())\n    print()\n",
    "path": "backend/research_team.py",
    "model_usage_metrics": {
        "Duration": 13942455518,
        "OpenAiUsage": {
            "completion_tokens": 0,
            "prompt_tokens": 0,
            "total_tokens": 0,
            "completion_tokens_details": {
                "accepted_prediction_tokens": 0,
                "audio_tokens": 0,
                "reasoning_tokens": 0,
                "rejected_prediction_tokens": 0
            },
            "prompt_tokens_details": {
                "audio_tokens": 0,
                "cached_tokens": 0
            }
        }
    }
}