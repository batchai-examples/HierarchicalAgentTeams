{
    "path": "backend/graph.py",
    "model_usage_metrics": {
        "Duration": 16732971077,
        "OpenAiUsage": {
            "completion_tokens": 0,
            "prompt_tokens": 0,
            "total_tokens": 0,
            "completion_tokens_details": {
                "accepted_prediction_tokens": 0,
                "audio_tokens": 0,
                "reasoning_tokens": 0,
                "rejected_prediction_tokens": 0
            },
            "prompt_tokens_details": {
                "audio_tokens": 0,
                "cached_tokens": 0
            }
        }
    },
    "test_file_path": "backend/test_graph.py",
    "existing_test_code": "",
    "original_code": "from typing import Literal\nimport asyncio\n\nfrom langchain_core.messages import HumanMessage, AIMessageChunk\nfrom langchain_openai import ChatOpenAI\n\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.types import Command\n\nfrom tools import make_supervisor_node\nfrom research_team import research_graph\nfrom paper_writing_team import paper_writing_graph\n\n##########################################################################################\n# Add Layers\n# \n# In this design, we are enforcing a top-down planning policy. We've created two graphs already, \n# but we have to decide how to route work between the two.\n#\n# We'll create a third graph to orchestrate the previous two, and add some connectors to \n# define how this top-level state is shared between the different graphs.\n\n\nllm = ChatOpenAI(model=\"gpt-4o\")\n\nteams_supervisor_node = make_supervisor_node(llm, [\"research_team\", \"writing_team\"])\n\ndef call_research_team(state: MessagesState) -\u003e Command[Literal[\"supervisor\"]]:\n    last_message = state[\"messages\"][-1]\n    response = research_graph.invoke({\"messages\": last_message})\n    \n    last_response = response[\"messages\"][-1].content\n    messages = [\n                HumanMessage(\n                    content=last_response, name=\"research_team\"\n                )\n            ]\n\n    return Command(\n        update={\n            \"messages\": messages\n        },\n        goto=\"supervisor\",\n    )\n\n\ndef call_paper_writing_team(state: MessagesState) -\u003e Command[Literal[\"supervisor\"]]:\n    last_message = state[\"messages\"][-1]\n    response = paper_writing_graph.invoke({\"messages\": last_message})\n\n    last_response = response[\"messages\"][-1].content\n    messages = [\n                HumanMessage(\n                    content=last_response, name=\"writing_team\"\n                )\n            ]\n\n    return Command(\n        update={\n            \"messages\": messages\n        },\n        goto=\"supervisor\",\n    )\n\n\n# Define the graph.\nsuper_builder = StateGraph(MessagesState)\nsuper_builder.add_node(\"supervisor\", teams_supervisor_node)\nsuper_builder.add_node(\"research_team\", call_research_team)\nsuper_builder.add_node(\"writing_team\", call_paper_writing_team)\n\nsuper_builder.add_edge(START, \"supervisor\")\nsuper_graph = super_builder.compile()\n\n\n# from IPython.display import Image\n\n# output_path = \"super_graph.png\" \n\n# png = Image(super_graph.get_graph().draw_mermaid_png())\n# png_data = png.data\n# with open(output_path, \"wb\") as file:\n#     file.write(png_data)\n\n# print(f\"Graph has been saved to {output_path}\")\n\nasync def test_graph():\n    async for messages in super_graph.astream(\n        {\n            \"messages\": [\n                (\"user\", \"Research AI agents and write a brief report about them.\")\n            ],\n        },\n        {\"recursion_limit\": 150},\n        stream_mode=\"messages\"\n    ):\n        #print(messages)\n        #print('------------------------')\n\n        checkpoint_ns:str = messages[1][\"checkpoint_ns\"]\n        is_cared_checkpoints = checkpoint_ns.startswith(\"search:\") or checkpoint_ns.startswith(\"note_taker:\")\n        if True or is_cared_checkpoints:\n            for msg in messages:\n                if isinstance(msg, AIMessageChunk):\n                    content = msg.content\n                    if content:\n                        print(content, end=\"\", flush=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(test_graph())\n    print()\n",
    "test_code": "import pytest\nfrom unittest.mock import patch, AsyncMock\nfrom backend.graph import call_research_team, call_paper_writing_team, super_graph\nfrom langchain_core.messages import HumanMessage\n\n# Test cases for the graph functions\n\n@pytest.mark.asyncio\nasync def test_call_research_team_happy_path():\n    \"\"\"\n    Test the call_research_team function with a valid state.\n    It should return a Command with updated messages and goto 'supervisor'.\n    \"\"\"\n    # Arrange\n    state = {\n        \"messages\": [\n            HumanMessage(content=\"Research AI agents and write a brief report about them.\", name=\"user\")\n        ]\n    }\n    \n    with patch('backend.research_graph.invoke', return_value={\"messages\": [HumanMessage(content=\"Research report generated.\", name=\"research_team\")]}) as mock_invoke:\n        # Act\n        command = await call_research_team(state)\n\n        # Assert\n        assert command.goto == \"supervisor\"\n        assert command.update[\"messages\"][0].content == \"Research report generated.\"\n        mock_invoke.assert_called_once()\n\n@pytest.mark.asyncio\nasync def test_call_paper_writing_team_happy_path():\n    \"\"\"\n    Test the call_paper_writing_team function with a valid state.\n    It should return a Command with updated messages and goto 'supervisor'.\n    \"\"\"\n    # Arrange\n    state = {\n        \"messages\": [\n            HumanMessage(content=\"Write a paper about AI advancements.\", name=\"user\")\n        ]\n    }\n    \n    with patch('backend.paper_writing_graph.invoke', return_value={\"messages\": [HumanMessage(content=\"Paper draft created.\", name=\"writing_team\")]}) as mock_invoke:\n        # Act\n        command = await call_paper_writing_team(state)\n\n        # Assert\n        assert command.goto == \"supervisor\"\n        assert command.update[\"messages\"][0].content == \"Paper draft created.\"\n        mock_invoke.assert_called_once()\n\n@pytest.mark.asyncio\nasync def test_call_research_team_no_messages():\n    \"\"\"\n    Test the call_research_team function with an empty messages state.\n    It should raise an IndexError since there are no messages to process.\n    \"\"\"\n    # Arrange\n    state = {\n        \"messages\": []\n    }\n    \n    with pytest.raises(IndexError):\n        await call_research_team(state)\n\n@pytest.mark.asyncio\nasync def test_call_paper_writing_team_no_messages():\n    \"\"\"\n    Test the call_paper_writing_team function with an empty messages state.\n    It should raise an IndexError since there are no messages to process.\n    \"\"\"\n    # Arrange\n    state = {\n        \"messages\": []\n    }\n    \n    with pytest.raises(IndexError):\n        await call_paper_writing_team(state)\n\n@pytest.mark.asyncio\nasync def test_super_graph_stream():\n    \"\"\"\n    Test the super_graph's async stream method with valid input.\n    It should yield messages containing AIMessageChunk.\n    \"\"\"\n    # Arrange\n    input_data = {\n        \"messages\": [\n            (\"user\", \"Research AI agents and write a brief report about them.\")\n        ]\n    }\n    \n    async def mock_astream(*args, **kwargs):\n        yield [{\"checkpoint_ns\": \"search:1\"}, {\"content\": \"AI agents researched.\", \"checkpoint_ns\": \"search:1\"}]\n\n    with patch.object(super_graph, 'astream', new_callable=AsyncMock, side_effect=mock_astream):\n        # Act\n        async for messages in super_graph.astream(input_data, {\"recursion_limit\": 150}, stream_mode=\"messages\"):\n            # Assert\n            assert len(messages) \u003e 0\n            assert \"content\" in messages[1]\n\n@pytest.mark.asyncio\nasync def test_super_graph_stream_no_messages():\n    \"\"\"\n    Test the super_graph's async stream method with no messages.\n    It should handle the case gracefully without yielding any messages.\n    \"\"\"\n    # Arrange\n    input_data = {\n        \"messages\": []\n    }\n    \n    async def mock_astream(*args, **kwargs):\n        yield []\n\n    with patch.object(super_graph, 'astream', new_callable=AsyncMock, side_effect=mock_astream):\n        # Act\n        async for messages in super_graph.astream(input_data, {\"recursion_limit\": 150}, stream_mode=\"messages\"):\n            # Assert\n            assert len(messages) == 0\n",
    "amount_of_generated_test_cases": 6,
    "single_test_run_command": "pytest backend/test_graph.py"
}